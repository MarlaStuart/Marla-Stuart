#######################Usual Care Clustering
# Kabakoff 2015, Chapter 16 Cluster Analysis

# Load the data
setwd("C:/Users/Marla Stuart/Dropbox/Usual Care/Data")
memory.limit(16000)
data = read.csv("ServicesAll.csv")
data = read.csv("TestData1000Services.csv")
names(data)[1] <- "ID" #Fix first variable name

# Choose attributes: 20 variables of PI and services
myvars <- names(data) %in% c("SchoolPI", "FamilyPI", "HealthPI", "SocialPI", "MentalPI", "OtherPI",
                                     "FamilySSS", "HealthSSS", "SocialSSS", "MentalSSS", "ShortSSS", "OtherSSS",
                                     "SchoolSSS", "SchoolSFM", "OtherSFM", "FamilyGRPS", "HealthGRPS", 
                                     "SocialGRPS", "MentalGRPS", "OtherGRPS") # select variables
cluster.data <- data[myvars]
class(cluster.data)
names(cluster.data)
dim(cluster.data)

# Scale the data: Not necessary, all the same scale already

# Screen for outliers because clustering can be sensitive to ouliers
# In binary data, outliers are not really a feature.
# But maybe there is a variable we're using that very rarely has values.
# If so, is that variable worth keeping?

apply(cluster.data, 2, sum) 

#family groups=26, health groups=74, and mental groups=83
#each <.006. Add to OtherGRPS

cluster.data$OtherGRPS <- cluster.data$OtherGRPS + cluster.data$FamilyGRPS + 
  cluster.data$HealthGRPS + cluster.data$MentalGRPS
myvars <- names(cluster.data) %in% c("FamilySSS", "HealthSSS", "SocialSSS", "MentalSSS", "ShortSSS", "OtherSSS",
                                     "SchoolSSS", "SchoolSFM", "OtherSFM", "SocialGRPS", "OtherGRPS")
cluster.data <- cluster.data[myvars]
class(cluster.data)
names(cluster.data)
dim(cluster.data)

# Assess clustering tendency
# Clustering tendency assessment determines whether a given dataset contains meaningful clusters 
# (i.e., non-random structure).
# Hopkins statistic is used to assess the clustering tendency of a dataset by measuring the probability '
# that a given dataset is generated by a uniform data distribution. In other words it tests the spatial 
# randomness of the data.
# http://www.sthda.com/english/wiki/assessing-clustering-tendency-a-vital-issue-unsupervised-machine-learning
# The null and the alternative hypotheses are defined as follow:
# Null hypothesis: the dataset D is uniformly distributed (i.e., no meaningful clusters)
# Alternative hypothesis: the dataset D is not uniformly distributed (i.e., contains meaningful clusters)
# If the value of Hopkins statistic is close to zero, then we can reject the null hypothesis and conclude 
# that the dataset D is significantly a clusterable data. Less than .5 is considered the threshhold.

# if(!require(devtools)) install.packages("devtools")
# devtools::install_github("kassambara/factoextra")
# install.packages("ggplot2")
library(clustertend)
library(factoextra)

set.seed(123)
# to check convergence of statistic, change n for repeated draws, which I did not do here
clustend <- get_clust_tendency(cluster.data, n = nrow(cluster.data)-1)
clustend$hopkins_stat #.411927
dev.off()
clustend$plot +
  scale_fill_gradient(low = "black", high = "white") +
  ggtitle("Visual Assessment of Cluster Tendency") +
  labs(x="Dark areas represent clusters")
pdf(file="Clusters")
dev.off()

# Select the optimal number of clusters
# Direct methods consists of optimizing a criterion, such as the within cluster sums of squares or the 
# average silhouette. The corresponding methods are named elbow and silhouette methods, respectively.
# Testing methods consists of comparing evidence against null hypothesis. An example is the gap statistic.
# elbow / wss

# 30 indices
# An important advantage of NbClust is that the user can simultaneously compute multiple indices and 
# determine the number of clusters in a single function call.
# The indices provided in NbClust package includes the gap statistic, the silhouette method and 28 other
# indices described comprehensively in the original paper of Charrad et al., 2014.
# 4 indices are timeconsuming to calculate -- so don't use them (all vs alllong)
# they are Gamma, Tau, Gap, and Gplus

library(NbClust)
nb <- NbClust(cluster.data, distance = "binary", max.nc = 20, method = "ward.D", index = "all")
fviz_nbclust(nb) + theme_minimal()

# Calculate distances.
# For binary variables Symmetric measures of distance consider 2 variables "similar"
# or "close" if they are both T or both F
# Asymmetric measures only view variables as being "close"
# if they are both T (Stat 133 Class Notes 2011, Spector 2011)
# Here we use asymmetric -- cases are close if 2 variables are true
# Can be similarity or dissimilarity
# Those that place each observation as own cluster are called agglomerative
# common choices for continuous data are Euclidean (example is Minkowski Metrik), correlation
# common choices for binary data are simple matching coefficient, Jaccard matching coefficient, 
# Phi-coefficient, binary, 
# common choices for mixed data are daisy (but may be better to make them all the same type)
# Jaccard only looks for common presences -- common absences are not considered important
# Here choose Jaccard using dist.binary

#require(ade4)
memory.limit(16000)
#d.jaccard <- dist.binary(cluster.data, method = 1)
d <- dist(cluster.data, method = "binary")

# Select a and run the clustering algorithm
# Choices are hierarchical or partitioning around mediods (PAM)
# Here hierarchical is not used becuase it is better for small datasets
# and it only accomodates Euclidean distances which are not appropriate for binary data
# PAM is used, but not k-means because it only accomodates euclidean distances which requires continuous variables

library(cluster)
set.seed(1234)
fit.pam <- eclust(cluster.data, "pam", k=4, hc_metric = "binary", hc_method = "ward.D2", graph = FALSE)

# Visualize the results
clusplot(fit.pam, main = "Bivariate Cluster Plot")
fviz_cluster(fit, geom = "point", frame.type = "norm")

# Validate the results
# Silhouette width
# Observations with large width (almost 1) are very well clustered
# Observations with small width (around 0) lie between two clusters
# Observations with negative widths are probably placed in the wrong cluster

fviz_silhouette(fit)

# identify cases in the wrong cluster (negative silhouette coefficient)
sil <- fit.pam$silinfo$widths[, 1:3]
neg_sil_index <- which(sil[, 'sil_width'] < 0)
sil[neg_sil_index, , drop = FALSE]

# Dunn index
# If the data set contains compact and well-separted clusters, the distance between clusters is large
# Therefore Dunn index should be large

library(fpc)
pam_stats <- cluster.stats(d, clustering = fit.pam2$clustering)

# export findings
cluster.data <- cbind(cluster.data, sil)
write.csv(cluster.data, file = "cluster.data") 
